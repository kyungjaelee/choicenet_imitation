{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChoiceNet for Robust Imitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is [1.10.0].\n"
     ]
    }
   ],
   "source": [
    "import nbloader,os,warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gym\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from sklearn.utils import shuffle\n",
    "from util import gpusession,create_gradient_clipping,data4imitation_noisy,data4imitation,data4imitation_expert_crazy,plot_1dRegData,print_n_txt\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "if __name__ == \"__main__\":\n",
    "    print (\"TensorFlow version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ChoiceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choiceNet_reg_class defined.\n"
     ]
    }
   ],
   "source": [
    "class choiceNet_imitation_class(object):\n",
    "    def __init__(self,_name='ChoiceNet',_scope='choicenet',_envname='HalfCheetah-v2',_xdim=1,_ydim=1,_hdims=[64,64]\n",
    "                 ,_kmix=5,_actv=tf.nn.relu,_bn=slim.batch_norm\n",
    "                 ,_rho_ref_train=0.95,_tau_inv=1e-2,_var_eps=1e-2\n",
    "                 ,_pi1_bias=0.0,_logSigmaZval=0\n",
    "                 ,_kl_reg_coef=1e-5,_l2_reg_coef=1e-5\n",
    "                 ,_SCHEDULE_MDN_REG=False\n",
    "                 ,_GPU_ID=0,_VERBOSE=True):\n",
    "        self.name = _name\n",
    "        self.scope = _scope\n",
    "        self.envname = _envname\n",
    "        self.env = gym.make(_envname)\n",
    "        self.xdim = _xdim\n",
    "        self.ydim = _ydim\n",
    "        self.hdims = _hdims\n",
    "        self.kmix = _kmix\n",
    "        self.actv = _actv \n",
    "        self.bn   = _bn # slim.batch_norm / None\n",
    "        self.rho_ref_train = _rho_ref_train # Rho for training \n",
    "        self.tau_inv = _tau_inv\n",
    "        self.var_eps = _var_eps # This will be used for the loss function (var+var_eps)\n",
    "        self.pi1_bias = _pi1_bias\n",
    "        self.logSigmaZval = _logSigmaZval\n",
    "        self.kl_reg_coef = _kl_reg_coef\n",
    "        self.l2_reg_coef = _l2_reg_coef # L2 regularizer \n",
    "        self.SCHEDULE_MDN_REG = _SCHEDULE_MDN_REG\n",
    "        self.GPU_ID = _GPU_ID\n",
    "        self.VERBOSE = _VERBOSE\n",
    "#         with tf.device('/device:CPU:%d'%(self.GPU_ID)):\n",
    "        # Build model\n",
    "        self.build_model()\n",
    "        # Build graph\n",
    "        self.build_graph()\n",
    "        # Check parameters\n",
    "        self.check_params()\n",
    "    # Build model\n",
    "    def build_model(self):\n",
    "        # Placeholders \n",
    "        self.x = tf.placeholder(dtype=tf.float32,shape=[None,self.xdim],name='x') # Input [None x xdim]\n",
    "        self.t = tf.placeholder(dtype=tf.float32,shape=[None,self.ydim],name='t') # Output [None x ydim]\n",
    "        self.kp = tf.placeholder(dtype=tf.float32,shape=[],name='kp') # Keep probability \n",
    "        self.lr = tf.placeholder(dtype=tf.float32,shape=[],name='lr') # Learning rate\n",
    "        self.is_training = tf.placeholder(dtype=tf.bool,shape=[]) # Training flag\n",
    "        self.rho_ref = tf.placeholder(dtype=tf.float32,shape=[],name='rho_ref') # Training flag\n",
    "        self.train_rate = tf.placeholder(dtype=tf.float32,shape=[],name='train_rate') # from 0.0~1.0\n",
    "        # Initializers\n",
    "        trni = tf.random_normal_initializer\n",
    "        tci = tf.constant_initializer\n",
    "        self.fully_init = trni(stddev=0.01)\n",
    "        self.bias_init = tci(0.)\n",
    "        self.bn_init = {'beta':tci(0.),'gamma':trni(1.,0.01)}\n",
    "        self.bn_params = {'is_training':self.is_training,'decay':0.9,'epsilon':1e-5,\n",
    "                           'param_initializers':self.bn_init,'updates_collections':None}\n",
    "        # Build graph\n",
    "        with tf.variable_scope(self.scope,self.name,reuse=False) as scope:\n",
    "            with slim.arg_scope([slim.fully_connected],activation_fn=self.actv,\n",
    "                                weights_initializer=self.fully_init,biases_initializer=self.bias_init,\n",
    "                                normalizer_fn=self.bn,normalizer_params=self.bn_params,\n",
    "                                weights_regularizer=None):\n",
    "                _net = self.x # Now we have an input\n",
    "                self.N = tf.shape(self.x)[0] # Input dimension\n",
    "                for h_idx in range(len(self.hdims)): # Loop over hidden layers\n",
    "                    _hdim = self.hdims[h_idx]\n",
    "                    _net = slim.fully_connected(_net,_hdim,scope='lin'+str(h_idx))\n",
    "                    _net = slim.dropout(_net,keep_prob=self.kp,is_training=self.is_training\n",
    "                                        ,scope='dr'+str(h_idx))\n",
    "                self.feat = _net # Feature [N x Q]\n",
    "                self.Q = self.feat.get_shape().as_list()[1] # Feature dimension\n",
    "                # Feature to K rhos (NO ACTIVATION !!!)\n",
    "                _rho_raw = slim.fully_connected(self.feat,self.kmix,activation_fn=None\n",
    "                                                ,scope='rho_raw')\n",
    "                \n",
    "                # self.rho_temp = tf.nn.tanh(_rho_raw) # [N x K] between -1.0~1.0 for regression\n",
    "                self.rho_temp = tf.nn.sigmoid(_rho_raw) # [N x K] between 0.0~1.0 for classification\n",
    "                \n",
    "                # Maker sure the first mixture to have 'self.rho_ref' correlation\n",
    "                self.rho = tf.concat([self.rho_temp[:,0:1]*0.0+self.rho_ref,self.rho_temp[:,1:]]\n",
    "                                     ,axis=1) # [N x K] \n",
    "                # Variabels for the sampler \n",
    "                self.muW = tf.get_variable(name='muW',shape=[self.Q,self.ydim],\n",
    "                                           initializer=tf.random_normal_initializer(stddev=0.1),\n",
    "                                           dtype=tf.float32) # [Q x D]\n",
    "                self.logSigmaW = tf.get_variable(name='logSigmaW'\n",
    "                                        ,shape=[self.Q,self.ydim]\n",
    "                                        ,initializer=tf.constant_initializer(-2.0)\n",
    "                                        ,dtype=tf.float32) # [Q x D]\n",
    "                self.muZ = tf.constant(np.zeros((self.Q,self.ydim))\n",
    "                                        ,name='muZ',dtype=tf.float32) # [Q x D]\n",
    "                self.logSigmaZ = tf.constant(self.logSigmaZval*np.ones((self.Q,self.ydim)) \n",
    "                                        ,name='logSigmaZ',dtype=tf.float32) # [Q x D]\n",
    "                # Reparametrization track (THIS PART IS COMPLICATED, I KNOW)\n",
    "                _muW_tile = tf.tile(self.muW[tf.newaxis,:,:]\n",
    "                                    ,multiples=[self.N,1,1]) # [N x Q x D]\n",
    "                _sigmaW_tile = tf.exp(tf.tile(self.logSigmaW[tf.newaxis,:,:]\n",
    "                                              ,multiples=[self.N,1,1])) # [N x Q x D]\n",
    "                _muZ_tile = tf.tile(self.muZ[tf.newaxis,:,:]\n",
    "                                    ,multiples=[self.N,1,1]) # [N x Q x D]\n",
    "                _sigmaZ_tile = tf.exp(tf.tile(self.logSigmaZ[tf.newaxis,:,:]\n",
    "                                              ,multiples=[self.N,1,1])) # [N x Q x D]\n",
    "                _samplerList = []\n",
    "                for jIdx in range(self.kmix): # For all K mixtures\n",
    "                    _rho_j = self.rho[:,jIdx:jIdx+1] # [N x 1] \n",
    "                    _rho_tile = tf.tile(_rho_j[:,:,tf.newaxis]\n",
    "                                        ,multiples=[1,self.Q,self.ydim]) # [N x Q x D]\n",
    "                    _epsW = tf.random_normal(shape=[self.N,self.Q,self.ydim],mean=0,stddev=1\n",
    "                                             ,dtype=tf.float32) # [N x Q x D]\n",
    "                    _W = _muW_tile + tf.sqrt(_sigmaW_tile)*_epsW # [N x Q x D]\n",
    "                    _epsZ = tf.random_normal(shape=[self.N,self.Q,self.ydim]\n",
    "                                             ,mean=0,stddev=1,dtype=tf.float32) # [N x Q x D]\n",
    "                    _Z = _muZ_tile + tf.sqrt(_sigmaZ_tile)*_epsZ # [N x Q x D]\n",
    "                    _Y = _rho_tile*_muW_tile + (1.0-_rho_tile**2) \\\n",
    "                        *(_rho_tile*tf.sqrt(_sigmaZ_tile)/tf.sqrt(_sigmaW_tile) \\\n",
    "                              *(_W-_muW_tile)+tf.sqrt(1-_rho_tile**2)*_Z)\n",
    "                    _samplerList.append(_Y) # Append \n",
    "                WlistConcat = tf.convert_to_tensor(_samplerList) # K*[N x Q x D] => [K x N x Q x D]\n",
    "                self.wSample = tf.transpose(WlistConcat,perm=[1,3,0,2]) # [N x D x K x Q]\n",
    "                # K mean mixtures [N x D x K]\n",
    "                _wTemp = tf.reshape(self.wSample\n",
    "                                ,shape=[self.N,self.kmix*self.ydim,self.Q]) # [N x KD x Q]\n",
    "                _featRsh = tf.reshape(self.feat,shape=[self.N,self.Q,1]) # [N x Q x 1]\n",
    "                _mu = tf.matmul(_wTemp,_featRsh) # [N x KD x Q] x [N x Q x 1] => [N x KD x 1]\n",
    "                self.mu = tf.reshape(_mu,shape=[self.N,self.ydim,self.kmix]) # [N x D x K]\n",
    "                # K variance mixtures [N x D x K]\n",
    "                _logvar_raw = slim.fully_connected(self.feat,self.ydim,scope='var_raw') # [N x D]\n",
    "                _var_raw = tf.exp(_logvar_raw) # [N x D]\n",
    "                _var_tile = tf.tile(_var_raw[:,:,tf.newaxis]\n",
    "                                    ,multiples=[1,1,self.kmix]) # [N x D x K]\n",
    "                _rho_tile = tf.tile(self.rho[:,tf.newaxis,:]\n",
    "                                    ,multiples=[1,self.ydim,1]) # [N x D x K]\n",
    "                _tau_inv = self.tau_inv\n",
    "                self.var = (1.0-_rho_tile**2)*_var_tile + _tau_inv # [N x D x K]\n",
    "                # Weight allocation probability pi [N x K]\n",
    "                _pi_logits = slim.fully_connected(self.feat,self.kmix\n",
    "                                                  ,scope='pi_logits') # [N x K]\n",
    "                self.pi_temp = tf.nn.softmax(_pi_logits,dim=1) # [N x K]\n",
    "                # Some heuristics to ensure that pi_1(x) is high enough\n",
    "                if self.pi1_bias != 0:\n",
    "                    self.pi_temp = tf.concat([self.pi_temp[:,0:1]+self.pi1_bias\n",
    "                                              ,self.pi_temp[:,1:]],axis=1) # [N x K]\n",
    "                    self.pi = tf.nn.softmax(self.pi_temp,dim=1) # [N x K]\n",
    "                else: self.pi = self.pi_temp # [N x K]\n",
    "    # Build graph\n",
    "    def build_graph(self):\n",
    "        # Parse\n",
    "        _M = tf.shape(self.x)[0] # Current batch size\n",
    "        t,pi,mu,var = self.t,self.pi,self.mu,self.var\n",
    "        \n",
    "        # Mixture density network loss \n",
    "        trepeat = tf.tile(t[:,:,tf.newaxis],[1,1,self.kmix]) # (N x D x K)\n",
    "        self.quadratics = -0.5*tf.reduce_sum(((trepeat-mu)**2)/(var+self.var_eps),axis=1) # (N x K)\n",
    "        self.logdet = -0.5*tf.reduce_sum(tf.log(var+self.var_eps),axis=1) # (N x K)\n",
    "        self.logconstant = - 0.5*self.ydim*tf.log(2*np.pi) # (1)\n",
    "        self.logpi = tf.log(pi) # (N x K)\n",
    "        self.exponents = self.quadratics + self.logdet + self.logpi # + self.logconstant \n",
    "        self.logprobs = tf.reduce_logsumexp(self.exponents,axis=1) # (N)\n",
    "        self.gmm_prob = tf.exp(self.logprobs) # (N)\n",
    "        self.gmm_nll  = -tf.reduce_mean(self.logprobs) # (1)\n",
    "        \n",
    "        # Regression loss \n",
    "        maxIdx = tf.argmax(input=pi,axis=1, output_type=tf.int32) # Argmax Index [N]\n",
    "        maxIdx = 0*tf.ones_like(maxIdx)\n",
    "        coords = tf.stack([tf.transpose(gv) for gv in tf.meshgrid(tf.range(self.N),tf.range(self.ydim))] + \n",
    "                          [tf.reshape(tf.tile(maxIdx[:,tf.newaxis],[1,self.ydim]),shape=(self.N,self.ydim))]\n",
    "                          ,axis=2) # [N x D x 3]\n",
    "        self.mu_bar = tf.gather_nd(mu,coords) # [N x D]\n",
    "        fit_mse_coef = 1e-2\n",
    "        self.fit_mse = fit_mse_coef*tf.maximum((1.0-2.0*self.train_rate),0.0) \\\n",
    "            *tf.reduce_sum(tf.pow(self.mu_bar-self.t,2))/(tf.cast(self.N,tf.float32)) # (1)\n",
    "        \n",
    "        # KL-divergence\n",
    "        _eps = 1e-2\n",
    "        self.rho_pos = self.rho+1.0 # Make it positive\n",
    "        self._kl_reg = self.kl_reg_coef*tf.reduce_sum(-self.rho_pos\n",
    "                        *(tf.log(self.pi+_eps)-tf.log(self.rho_pos+_eps)),axis=1) # (N)\n",
    "        self.kl_reg = tf.reduce_mean(self._kl_reg) # (1)\n",
    "\n",
    "        # Weight decay\n",
    "        _g_vars = tf.trainable_variables()\n",
    "        self.c_vars = [var for var in _g_vars if '%s/'%(self.scope) in var.name]\n",
    "        self.l2_reg = self.l2_reg_coef*tf.reduce_sum(tf.stack([tf.nn.l2_loss(v) for v in self.c_vars])) # [1]\n",
    "\n",
    "        # Schedule MDN loss and regression loss \n",
    "        if self.SCHEDULE_MDN_REG:\n",
    "            self.gmm_nll = tf.minimum((2.0*self.train_rate+0.1),1.0)*self.gmm_nll\n",
    "            self.fit_mse = tf.maximum((1.0-2.0*self.train_rate),0.0)*self.fit_mse\n",
    "            self.loss_total = self.gmm_nll+self.kl_reg+self.l2_reg+self.fit_mse # [1]\n",
    "        else:\n",
    "            self.gmm_nll = self.gmm_nll\n",
    "            self.fit_mse = tf.constant(0.0)\n",
    "            self.loss_total = self.gmm_nll+self.kl_reg+self.l2_reg\n",
    "        \n",
    "        # Optimizer\n",
    "        USE_ADAM = False\n",
    "        GRAD_CLIP = True\n",
    "        if GRAD_CLIP: # Gradient clipping\n",
    "            if USE_ADAM:\n",
    "                _optm = tf.train.AdamOptimizer(learning_rate=self.lr\n",
    "                                               ,beta1=0.9,beta2=0.999,epsilon=1e-1) # 1e-4\n",
    "            else:\n",
    "                _optm = tf.train.MomentumOptimizer(learning_rate=self.lr,momentum=0.0)\n",
    "            self.optm = create_gradient_clipping(self.loss_total\n",
    "                                           ,_optm,tf.trainable_variables(),clipVal=1.0)\n",
    "        else:\n",
    "            if USE_ADAM:\n",
    "                self.optm = tf.train.AdamOptimizer(learning_rate=self.lr\n",
    "                            ,beta1=0.9,beta2=0.999,epsilon=1e-1).minimize(self.loss_total) \n",
    "            else:\n",
    "                self.optm = tf.train.MomentumOptimizer(learning_rate=self.lr\n",
    "                                                       ,momentum=0.0).minimize(self.loss_total)\n",
    "                \n",
    "    # Check parameters\n",
    "    def check_params(self):\n",
    "        _g_vars = tf.global_variables()\n",
    "        self.g_vars = [var for var in _g_vars if '%s/'%(self.scope) in var.name]\n",
    "        if self.VERBOSE:\n",
    "            print (\"==== Global Variables ====\")\n",
    "        for i in range(len(self.g_vars)):\n",
    "            w_name  = self.g_vars[i].name\n",
    "            w_shape = self.g_vars[i].get_shape().as_list()\n",
    "            if self.VERBOSE:\n",
    "                print (\" [%02d] Name:[%s] Shape:[%s]\" % (i,w_name,w_shape))\n",
    "    \n",
    "    # Sampler\n",
    "    def sampler(self,_sess,_x,n_samples=1,_deterministic=True):\n",
    "        pi, mu, var = _sess.run([self.pi, self.mu, self.var],\n",
    "                                feed_dict={self.x:_x,self.kp:1.0,self.is_training:False\n",
    "                                          ,self.rho_ref:1.0}) #\n",
    "        n_points = _x.shape[0]\n",
    "        _y_sampled = np.zeros([n_points,self.ydim,n_samples])\n",
    "        for i in range(n_points):\n",
    "            for j in range(n_samples):\n",
    "                if _deterministic: k = 0\n",
    "                else: k = np.random.choice(self.kmix,p=pi[i,:])\n",
    "                _y_sampled[i,:,j] = mu[i,:,k] # + np.random.randn(1,self.ydim)*np.sqrt(var[i,:,k])\n",
    "        return _y_sampled \n",
    "    \n",
    "    # Save \n",
    "    def save(self,_sess,_savename=None):\n",
    "        \"\"\" Save name \"\"\"\n",
    "        if _savename==None:\n",
    "            _savename='../net/imitation_net_%s.npz'%(self.scope)\n",
    "        \"\"\" Get global variables \"\"\"\n",
    "        self.g_wnames,self.g_wvals,self.g_wshapes = [],[],[]\n",
    "        for i in range(len(self.g_vars)):\n",
    "            curr_wname = self.g_vars[i].name\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==curr_wname][0]\n",
    "            curr_wval  = _sess.run(curr_wvar)\n",
    "            curr_wval_sqz  = curr_wval.squeeze()\n",
    "            self.g_wnames.append(curr_wname)\n",
    "            self.g_wvals.append(curr_wval_sqz)\n",
    "            self.g_wshapes.append(curr_wval.shape)\n",
    "        \"\"\" Save \"\"\"\n",
    "        np.savez(_savename,g_wnames=self.g_wnames,g_wvals=self.g_wvals,g_wshapes=self.g_wshapes)\n",
    "        if self.VERBOSE:\n",
    "            print (\"[%s] Saved. Size is [%.4f]MB\" % \n",
    "                   (_savename,os.path.getsize(_savename)/1000./1000.))\n",
    "    \n",
    "    # Save \n",
    "    def save_final(self,_sess,_savename=None):\n",
    "        \"\"\" Save name \"\"\"\n",
    "        if _savename==None:\n",
    "            _savename='../net/imitation_net_%s_final.npz'%(self.scope)\n",
    "        \"\"\" Get global variables \"\"\"\n",
    "        self.g_wnames,self.g_wvals,self.g_wshapes = [],[],[]\n",
    "        for i in range(len(self.g_vars)):\n",
    "            curr_wname = self.g_vars[i].name\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==curr_wname][0]\n",
    "            curr_wval  = _sess.run(curr_wvar)\n",
    "            curr_wval_sqz  = curr_wval.squeeze()\n",
    "            self.g_wnames.append(curr_wname)\n",
    "            self.g_wvals.append(curr_wval_sqz)\n",
    "            self.g_wshapes.append(curr_wval.shape)\n",
    "        \"\"\" Save \"\"\"\n",
    "        np.savez(_savename,g_wnames=self.g_wnames,g_wvals=self.g_wvals,g_wshapes=self.g_wshapes)\n",
    "        print (\"[%s] Saved. Size is [%.4f]MB\" % \n",
    "               (_savename,os.path.getsize(_savename)/1000./1000.))\n",
    "        \n",
    "    # Restore\n",
    "    def restore(self,_sess,_loadname=None):\n",
    "        if _loadname==None:\n",
    "            _loadname='../net/imitation_net_%s_final.npz'%(self.scope)\n",
    "        l = np.load(_loadname)\n",
    "        g_wnames = l['g_wnames']\n",
    "        g_wvals  = l['g_wvals']\n",
    "        g_wshapes = l['g_wshapes']\n",
    "        for widx,wname in enumerate(g_wnames):\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==wname][0]\n",
    "            _sess.run(tf.assign(curr_wvar,g_wvals[widx].reshape(g_wshapes[widx])))\n",
    "        if self.VERBOSE:\n",
    "            print (\"Weight restored from [%s] Size is [%.4f]MB\" % \n",
    "                   (_loadname,os.path.getsize(_loadname)/1000./1000.))\n",
    "    \n",
    "    # Save to mat file\n",
    "    def save2mat(self,_xdata='',_ydata='',_yref=''):\n",
    "        # Save weights to mat file so that MATLAB can use it.\n",
    "        npzPath = '../net/imitation_net_%s.npz'%(self.scope)\n",
    "        l = np.load(npzPath)\n",
    "        g_wnames = l['g_wnames']\n",
    "        g_wvals  = l['g_wvals']\n",
    "        g_wshapes = l['g_wshapes']\n",
    "        D = {}\n",
    "        for widx,wname in enumerate(g_wnames):\n",
    "            cName = wname.replace(':0','')\n",
    "            cName = cName.replace(self.scope+'/','')\n",
    "            cName = cName.replace('/','_')\n",
    "            cVal = g_wvals[widx].reshape(g_wshapes[widx])\n",
    "            D[cName] = cVal\n",
    "            # if self.VERBOSE: print (\"name is [%s] shape is %s.\"%(cName,cVal.shape,))\n",
    "        # Save data\n",
    "        if _xdata!='': D['xdata']=_xdata\n",
    "        if _ydata!='': D['ydata']=_ydata\n",
    "        if _yref!='': D['yref']=_yref\n",
    "        # Save dictionary D to the mat file\n",
    "        matPath = '../data/imitation_net_%s.mat'%(self.scope)\n",
    "        if self.VERBOSE: print (\"[%s] saved.\"%(matPath))\n",
    "        sio.savemat(matPath,D)\n",
    "        \n",
    "    # Save to mat file\n",
    "    def save2mat_final(self,_xdata='',_ydata='',_yref=''):\n",
    "        # Save weights to mat file so that MATLAB can use it.\n",
    "        npzPath = '../net/imitation_net_%s_final.npz'%(self.scope)\n",
    "        l = np.load(npzPath)\n",
    "        g_wnames = l['g_wnames']\n",
    "        g_wvals  = l['g_wvals']\n",
    "        g_wshapes = l['g_wshapes']\n",
    "        D = {}\n",
    "        for widx,wname in enumerate(g_wnames):\n",
    "            cName = wname.replace(':0','')\n",
    "            cName = cName.replace(self.scope+'/','')\n",
    "            cName = cName.replace('/','_')\n",
    "            cVal = g_wvals[widx].reshape(g_wshapes[widx])\n",
    "            D[cName] = cVal\n",
    "            if self.VERBOSE:\n",
    "                print (\"name is [%s] shape is %s.\"%(cName,cVal.shape,))\n",
    "        # Save data\n",
    "        if _xdata!='': D['xdata']=_xdata\n",
    "        if _ydata!='': D['ydata']=_ydata\n",
    "        if _yref!='': D['yref']=_yref\n",
    "        # Save dictionary D to the mat file\n",
    "        matPath = '../data/imitation_%s_final.mat'%(self.scope)\n",
    "        sio.savemat(matPath,D)\n",
    "        print (\"[%s] Saved. Size is [%.4f]MB\" % \n",
    "               (matPath,os.path.getsize(matPath)/1000./1000.))\n",
    "    \n",
    "    # Train\n",
    "    def train(self,_sess,_x,_y,_yref='',_lr=1e-3,_batchSize=512,_maxEpoch=1e4,_kp=1.0\n",
    "              ,_LR_SCHEDULE=True\n",
    "              ,_PRINT_EVERY=20,_PLOT_EVERY=20\n",
    "              ,_SAVE_TXT=True,_SAVE_BEST_NET=True,_SAVE_FINAL=True):\n",
    "        \n",
    "        # Reference training data \n",
    "        _x_train,_y_train = _x,_y\n",
    "        \n",
    "        # Iterate\n",
    "        if _PRINT_EVERY == 0: print_period = 0\n",
    "        else: print_period = _maxEpoch//_PRINT_EVERY\n",
    "        if _PLOT_EVERY == 0: plot_period = 0\n",
    "        else: plot_period = _maxEpoch//_PLOT_EVERY\n",
    "            \n",
    "        maxIter = max(_x_train.shape[0]//_batchSize, 1)\n",
    "        bestLossVal = np.inf\n",
    "        if _SAVE_TXT:\n",
    "            txtName = ('../res/imitation_%s.txt'%(self.name));f = open(txtName,'w') # Open txt file\n",
    "            print_n_txt(_f=f,_chars='Text name: '+txtName,_DO_PRINT=True)\n",
    "        for epoch in range((int)(_maxEpoch)+1): # For every epoch\n",
    "            train_rate = (float)(epoch/_maxEpoch)\n",
    "            _x_train,_y_train = shuffle(_x_train,_y_train)\n",
    "            for iter in range(maxIter): # For every iteration\n",
    "                start,end = iter*_batchSize,(iter+1)*_batchSize\n",
    "                if _LR_SCHEDULE:\n",
    "                    if epoch < 0.5*_maxEpoch:\n",
    "                        lr_use = _lr\n",
    "                    elif epoch < 0.75*_maxEpoch:\n",
    "                        lr_use = _lr/10.\n",
    "                    else:\n",
    "                        lr_use = _lr/100.\n",
    "                else:\n",
    "                    lr_use = _lr\n",
    "                feeds = {self.x:_x_train[start:end,:],self.t:_y_train[start:end,:]\n",
    "                         ,self.kp:_kp,self.lr:lr_use,self.train_rate:(float)(epoch/_maxEpoch)\n",
    "                         ,self.rho_ref:self.rho_ref_train,self.is_training:True}\n",
    "                # Optimize \n",
    "                _sess.run(self.optm,feeds)\n",
    "\n",
    "            # Track the Best result\n",
    "            BEST_FLAG = False\n",
    "            check_period = _maxEpoch//100\n",
    "            if (epoch%check_period)==0:\n",
    "                feeds = {self.x:_x,self.t:_y,self.kp:1.0,self.train_rate:train_rate\n",
    "                         ,self.rho_ref:self.rho_ref_train,self.is_training:False}\n",
    "                opers = [self.loss_total,self.gmm_nll,self.kl_reg,self.l2_reg,self.fit_mse]\n",
    "                lossVal,gmm_nll,kl_reg,l2_reg,fit_mse = _sess.run(opers,feeds)\n",
    "                if (lossVal < bestLossVal) & (train_rate >= 0.5):\n",
    "                    bestLossVal = lossVal\n",
    "                    BEST_FLAG = True\n",
    "                    if _SAVE_BEST_NET:\n",
    "                        self.save(_sess) # Save the current best model \n",
    "                        self.save2mat(_xdata=_x,_ydata=_y,_yref=_yref)\n",
    "            \n",
    "            # Print current result \n",
    "            if (print_period!=0) and ((epoch%print_period)==0 or (epoch==(_maxEpoch-1))): # Print \n",
    "                # Feed total dataset \n",
    "                feeds = {self.x:_x,self.t:_y,self.kp:1.0,self.train_rate:(float)(epoch/_maxEpoch)\n",
    "                         ,self.rho_ref:self.rho_ref_train,self.is_training:False}\n",
    "                opers = [self.loss_total,self.gmm_nll,self.kl_reg,self.l2_reg,self.fit_mse]\n",
    "                lossVal,gmm_nll,kl_reg,l2_reg,fit_mse = _sess.run(opers,feeds)\n",
    "                if _SAVE_TXT:\n",
    "                    strTemp = (\"[%d/%d] loss:%.3f(gmm:%.3f+kl:%.3f+l2:%.3f+fit:%.3f) bestLoss:%.3f\"\n",
    "                               %(epoch,_maxEpoch,lossVal,gmm_nll,kl_reg,l2_reg,fit_mse,bestLossVal))\n",
    "                    print_n_txt(_f=f,_chars=strTemp,_DO_PRINT=self.VERBOSE)\n",
    "                else:\n",
    "                    if self.VERBOSE:\n",
    "                        print (\"[%d/%d] loss:%.3f(gmm:%.3f+kl:%.3f+l2:%.3f+fit:%.3f) bestLoss:%.3f\"\n",
    "                                   %(epoch,_maxEpoch,lossVal,gmm_nll,kl_reg,l2_reg,fit_mse,bestLossVal))\n",
    "\n",
    "            # Plot current result \n",
    "            if (plot_period!=0) and ((epoch%plot_period)==0 or (epoch==(_maxEpoch-1))): # Plot\n",
    "                # Get loss values\n",
    "                feeds = {self.x:_x,self.t:_y,self.kp:1.0,self.train_rate:(float)(epoch/_maxEpoch)\n",
    "                         ,self.rho_ref:self.rho_ref_train,self.is_training:False}\n",
    "                opers = [self.loss_total,self.gmm_nll,self.kl_reg,self.l2_reg,self.fit_mse]\n",
    "                lossVal,gmm_nll,kl_reg,l2_reg,fit_mse = _sess.run(opers,feeds)\n",
    "                # Sampling\n",
    "                nSample = 1\n",
    "                ytest = self.sampler(_sess=_sess,_x=_x,n_samples=nSample)\n",
    "                # Plot first dimensions of both input and output\n",
    "                x_plot,y_plot = _x[:,0],_y[:,0] # Traning data \n",
    "                plt.figure(figsize=(8,4));\n",
    "                plt.axis([np.min(x_plot),np.max(x_plot),np.min(y_plot)-0.1,np.max(y_plot)+0.1])\n",
    "                if _yref != '': plt.plot(x_plot,_yref[:,0],'r.') # Plot reference\n",
    "                plt.plot(x_plot,y_plot,'k.') # Plot training data\n",
    "                for i in range(nSample): plt.plot(_x,ytest[:,0,i],'b.')\n",
    "                plt.title(\"[%d/%d] name:[%s] lossVal:[%.3e]\"%(epoch,_maxEpoch,self.name,lossVal)); \n",
    "                plt.show()\n",
    "                \n",
    "        # Save final weights \n",
    "        if _SAVE_FINAL:\n",
    "            self.save_final(_sess)\n",
    "            self.save2mat_final(_xdata=_x,_ydata=_y,_yref=_yref)\n",
    "    \n",
    "    def evaluate_episode(self, _sess, animate=False): # Run policy and collect (state, action, reward) pairs\n",
    "        env = self.env\n",
    "        obs = env.reset()\n",
    "        observes, actions, rewards, infos = [], [], [], []\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            obs = obs.astype(np.float32).reshape((1, -1))\n",
    "            observes.append(obs)\n",
    "\n",
    "            action = self.sampler(_sess=_sess,_x=obs,n_samples=1).reshape((1,-1))\n",
    "            actions.append(action)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            if not isinstance(reward, float):\n",
    "                reward = np.asscalar(reward)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "        \n",
    "        return (np.concatenate(observes), np.array(actions), np.array(rewards, dtype=np.float32), infos)\n",
    "\n",
    "    def evaluate_policy(self, _sess, episodes): # collect trajectories\n",
    "        total_steps = 0\n",
    "        trajectories = []\n",
    "        for e in range(episodes):\n",
    "            observes, actions, rewards, infos = self.evaluate_episode(_sess)\n",
    "            total_steps += observes.shape[0]\n",
    "            trajectory = {'observes': observes,\n",
    "                          'actions': actions,\n",
    "                          'rewards': rewards,\n",
    "                          'infos': infos}\n",
    "            trajectories.append(trajectory)\n",
    "        return trajectories\n",
    "    \n",
    "    # Test\n",
    "    def test(self,_sess,_PLOT_TRAIN=True,_PLOT_RES=True,_SAVE_FIG=False):\n",
    "        \n",
    "        demonstrations = self.evaluate_policy(_sess, 100)\n",
    "        avg_ret = np.mean([np.sum(d['rewards']) for d in demonstrations])\n",
    "        std_ret = np.std([np.sum(d['rewards']) for d in demonstrations])\n",
    "        print('Avg Return:%.02f (%.02f)'%(avg_ret,std_ret))\n",
    "        with open('../res/%s.pickle'%(self.name),'wb') as f:\n",
    "            pickle.dump({'avg_ret':avg_ret,'std_ret':std_ret},f)\n",
    "            f.close()\n",
    "        return avg_ret\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    print (\"choiceNet_reg_class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "WARNING:tensorflow:From <ipython-input-2-688838d78bdd>:133: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Training data \n",
    "    oRate = [1.0,0.0]\n",
    "    measVar = 1e-8\n",
    "    x,y,t=data4imitation_expert_crazy(envname='Walker2d-v2',_n=500,_oRate=oRate,measVar=measVar)\n",
    "\n",
    "    # Make graph \n",
    "    tf.reset_default_graph(); sess = gpusession()\n",
    "    tf.set_random_seed(0); np.random.seed(0)\n",
    "    CN = choiceNet_imitation_class(_name='CN_%s_E%02d_C%02d_var%.1e'%('Hopper',oRate[0]*100,oRate[1]*100,measVar),_envname='Walker2d-v2'\n",
    "                            ,_xdim=x.shape[1],_ydim=y.shape[1],_hdims=[64,64]    \n",
    "                            ,_kmix=5,_actv=tf.nn.relu,_bn=slim.batch_norm\n",
    "                            ,_rho_ref_train=0.99,_tau_inv=1e-2,_var_eps=1e-4\n",
    "                            ,_pi1_bias=0.0,_logSigmaZval=0\n",
    "                            ,_kl_reg_coef=1e-5,_l2_reg_coef=1e-5\n",
    "                            ,_SCHEDULE_MDN_REG=False\n",
    "                            ,_GPU_ID=0,_VERBOSE=False)\n",
    "    sess.run(tf.global_variables_initializer()) # Initialize variables\n",
    "    \n",
    "    # Train \n",
    "    DO_TRAIN = True\n",
    "    if DO_TRAIN:\n",
    "        CN.train(_sess=sess,_x=x,_y=y,_yref=t\n",
    "               ,_lr=1e-3,_batchSize=256,_maxEpoch=1e4,_kp=1.0\n",
    "               ,_LR_SCHEDULE=False \n",
    "               ,_PRINT_EVERY=20,_PLOT_EVERY=20\n",
    "               ,_SAVE_TXT=True,_SAVE_BEST_NET=True)\n",
    "        print (\"Train done.\")\n",
    "    else:\n",
    "        CN.restore(sess)\n",
    "        print (\"Network restored.\")\n",
    "    # Test \n",
    "    avg_ret=CN.test(_sess=sess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
