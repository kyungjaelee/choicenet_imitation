{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOAgent_gaussian(object):\n",
    "    def __init__(self, obs_dim, act_dim, clip_range=0.2, epochs=10, policy_lr=3e-4, value_lr=7e-4, hdim=64, max_std=1.0, seed=0):\n",
    "        \n",
    "        self.seed=seed\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.act_dim = act_dim\n",
    "        \n",
    "        self.clip_range = clip_range\n",
    "        \n",
    "        self.epochs = epochs\n",
    "        self.policy_lr = policy_lr\n",
    "        self.value_lr = value_lr\n",
    "        self.hdim = hdim\n",
    "        self.max_std = max_std\n",
    "        \n",
    "        self._build_graph()\n",
    "        self._init_session()\n",
    "        \n",
    "    def _build_graph(self):\n",
    "        self.g = tf.Graph()\n",
    "        with self.g.as_default():\n",
    "            self._placeholders()\n",
    "            self._policy_nn()\n",
    "            self._value_nn()\n",
    "            self._logprob()\n",
    "            self._loss_train_op()\n",
    "            self._kl_entropy()\n",
    "            self.init = tf.global_variables_initializer()\n",
    "            self.variables = tf.global_variables()\n",
    "            \n",
    "    def _placeholders(self):\n",
    "        # observations, actions and advantages:\n",
    "        self.obs_ph = tf.placeholder(tf.float32, (None, self.obs_dim), 'obs')\n",
    "        self.act_ph = tf.placeholder(tf.float32, (None, act_dim), 'act')\n",
    "        self.adv_ph = tf.placeholder(tf.float32, (None,), 'adv')\n",
    "        self.ret_ph = tf.placeholder(tf.float32, (None,), 'ret')\n",
    "\n",
    "        # learning rate:\n",
    "        self.policy_lr_ph = tf.placeholder(tf.float32, (), 'policy_lr')\n",
    "        self.value_lr_ph = tf.placeholder(tf.float32, (), 'value_lr')\n",
    "        \n",
    "        # place holder for old parameters\n",
    "        self.old_std_ph = tf.placeholder(tf.float32, (None, self.act_dim), 'old_std')\n",
    "        self.old_mean_ph = tf.placeholder(tf.float32, (None, self.act_dim), 'old_means')\n",
    "        \n",
    "    def _policy_nn(self):\n",
    "        \n",
    "        hid1_size = self.hdim\n",
    "        hid2_size = self.hdim\n",
    "        with tf.variable_scope(\"policy\"):\n",
    "            # TWO HIDDEN LAYERS\n",
    "            out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"h1\")\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), name=\"h2\")\n",
    "\n",
    "            # MEAN FUNCTION\n",
    "            self.mean = tf.layers.dense(out, self.act_dim,\n",
    "                                    kernel_initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed), \n",
    "                                    name=\"mean\")\n",
    "            # UNI-VARIATE\n",
    "            self.logits_std = tf.get_variable(\"logits_std\",shape=(1,),initializer=tf.random_normal_initializer(stddev=0.01,seed= self.seed))\n",
    "            self.std = self.max_std*tf.ones_like(self.mean)*tf.sigmoid(self.logits_std) # IMPORTANT TRICK\n",
    "\n",
    "            # SAMPLE OPERATION\n",
    "            self.sample_action = self.mean + tf.random_normal(tf.shape(self.mean),seed=self.seed)*self.std\n",
    "    \n",
    "    def _value_nn(self):\n",
    "        hid1_size = self.hdim \n",
    "        hid2_size = self.hdim\n",
    "        with tf.variable_scope(\"value\"):\n",
    "            out = tf.layers.dense(self.obs_ph, hid1_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h1\")\n",
    "            out = tf.layers.dense(out, hid2_size, tf.tanh,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name=\"h2\")\n",
    "            value = tf.layers.dense(out, 1,\n",
    "                                  kernel_initializer=tf.random_normal_initializer(\n",
    "                                      stddev=0.01,seed=self.seed), name='output')\n",
    "            self.value = tf.squeeze(value)\n",
    "            \n",
    "    def _logprob(self):\n",
    "        # PROBABILITY WITH TRAINING PARAMETER\n",
    "        y = self.act_ph \n",
    "        mu = self.mean\n",
    "        sigma = self.std\n",
    "        \n",
    "        self.logp = tf.reduce_sum(-0.5*tf.square((y-mu)/sigma)-tf.log(sigma)- 0.5*np.log(2.*np.pi),axis=1)\n",
    "\n",
    "        # PROBABILITY WITH OLD (PREVIOUS) PARAMETER\n",
    "        old_mu_ph = self.old_mean_ph\n",
    "        old_sigma_ph = self.old_std_ph\n",
    "                \n",
    "        self.logp_old = tf.reduce_sum(-0.5*tf.square((y-old_mu_ph)/old_sigma_ph)-tf.log(old_sigma_ph)- 0.5*np.log(2.*np.pi),axis=1)\n",
    "        \n",
    "    def _kl_entropy(self):\n",
    "\n",
    "        mean, std = self.mean, self.std\n",
    "        old_mean, old_std = self.old_mean_ph, self.old_std_ph\n",
    " \n",
    "        log_std_old = tf.log(old_std)\n",
    "        log_std_new = tf.log(std)\n",
    "        frac_std_old_new = old_std/std\n",
    "\n",
    "        # KL DIVERGENCE BETWEEN TWO GAUSSIAN\n",
    "        kl = tf.reduce_sum(log_std_new - log_std_old + 0.5*tf.square(frac_std_old_new) + 0.5*tf.square((mean - old_mean)/std)- 0.5,axis=1)\n",
    "        self.kl = tf.reduce_mean(kl)\n",
    "        \n",
    "        # ENTROPY OF GAUSSIAN\n",
    "        entropy = tf.reduce_sum(log_std_new + 0.5 + 0.5*np.log(2*np.pi),axis=1)\n",
    "        self.entropy = tf.reduce_mean(entropy)\n",
    "            \n",
    "    def _loss_train_op(self):\n",
    "        \n",
    "        # REINFORCE OBJECTIVE\n",
    "        ratio = tf.exp(self.logp - self.logp_old)\n",
    "        cliped_ratio = tf.clip_by_value(ratio,clip_value_min=1-self.clip_range,clip_value_max=1+self.clip_range)\n",
    "        self.policy_loss = -tf.reduce_mean(tf.minimum(self.adv_ph*ratio,self.adv_ph*cliped_ratio))\n",
    "        \n",
    "        # POLICY OPTIMIZER\n",
    "        self.pol_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"policy\")\n",
    "        optimizer = tf.train.AdamOptimizer(self.policy_lr_ph)\n",
    "        self.train_policy = optimizer.minimize(self.policy_loss,var_list=self.pol_var_list)\n",
    "            \n",
    "        # L2 LOSS\n",
    "        self.value_loss = tf.reduce_mean(0.5*tf.square(self.value - self.ret_ph))\n",
    "            \n",
    "        # VALUE OPTIMIZER \n",
    "        self.val_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"value\")\n",
    "        optimizer = tf.train.AdamOptimizer(self.value_lr_ph)\n",
    "        self.train_value = optimizer.minimize(self.value_loss,var_list=self.val_var_list)\n",
    "        \n",
    "    def _init_session(self):\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        self.sess = tf.Session(config=config,graph=self.g)\n",
    "        self.sess.run(self.init)\n",
    "    \n",
    "    def get_value(self, obs):\n",
    "        feed_dict = {self.obs_ph: obs}\n",
    "        value = self.sess.run(self.value, feed_dict=feed_dict)\n",
    "        return value\n",
    "    \n",
    "    def get_action(self, obs): # SAMPLE FROM POLICY\n",
    "        feed_dict = {self.obs_ph: obs}\n",
    "        sampled_action = self.sess.run(self.sample_action,feed_dict=feed_dict)\n",
    "        return sampled_action[0]\n",
    "    \n",
    "    def control(self, obs): # COMPUTE MEAN\n",
    "        feed_dict = {self.obs_ph: obs}\n",
    "        best_action = self.sess.run(self.mean,feed_dict=feed_dict)\n",
    "        return best_action    \n",
    "    \n",
    "    def update(self, observes, actions, advantages, returns, batch_size = 128): # TRAIN POLICY\n",
    "        \n",
    "        num_batches = max(observes.shape[0] // batch_size, 1)\n",
    "        batch_size = observes.shape[0] // num_batches\n",
    "        \n",
    "        old_means_np, old_std_np = self.sess.run([self.mean, self.std],{self.obs_ph: observes}) # COMPUTE OLD PARAMTER\n",
    "        for e in range(self.epochs):\n",
    "            observes, actions, advantages, returns, old_means_np, old_std_np = shuffle(observes, actions, advantages, returns, old_means_np, old_std_np, random_state=self.seed)\n",
    "            for j in range(num_batches): \n",
    "                start = j * batch_size\n",
    "                end = (j + 1) * batch_size\n",
    "                feed_dict = {self.obs_ph: observes[start:end,:],\n",
    "                     self.act_ph: actions[start:end],\n",
    "                     self.adv_ph: advantages[start:end],\n",
    "                     self.ret_ph: returns[start:end],\n",
    "                     self.old_std_ph: old_std_np[start:end,:],\n",
    "                     self.old_mean_ph: old_means_np[start:end,:],\n",
    "                     self.policy_lr_ph: self.policy_lr,\n",
    "                     self.value_lr_ph: self.value_lr}        \n",
    "                self.sess.run([self.train_policy,self.train_value], feed_dict)\n",
    "            \n",
    "        feed_dict = {self.obs_ph: observes,\n",
    "             self.act_ph: actions,\n",
    "             self.adv_ph: advantages,\n",
    "             self.ret_ph: returns,\n",
    "             self.old_std_ph: old_std_np,\n",
    "             self.old_mean_ph: old_means_np,\n",
    "             self.policy_lr_ph: self.policy_lr,\n",
    "             self.value_lr_ph: self.value_lr}               \n",
    "        policy_loss, value_loss, kl, entropy  = self.sess.run([self.policy_loss, self.value_loss, self.kl, self.entropy], feed_dict)\n",
    "        return policy_loss, value_loss, kl, entropy\n",
    "    \n",
    "    def close_sess(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_episode(env, policy, animate=False): # Run policy and collect (state, action, reward) pairs\n",
    "    obs = env.reset()\n",
    "    observes, actions, rewards, infos = [], [], [], []\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        obs = obs.astype(np.float32).reshape((1, -1))\n",
    "        observes.append(obs)\n",
    "        \n",
    "        action = agent.control(obs).reshape((1, -1))\n",
    "        actions.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        \n",
    "        if not isinstance(reward, float):\n",
    "            reward = np.asscalar(reward)\n",
    "        rewards.append(reward)\n",
    "        infos.append(info)\n",
    "        \n",
    "    return (np.concatenate(observes), np.array(actions), np.array(rewards, dtype=np.float32), infos)\n",
    "\n",
    "def evaluate_policy(env, agent, episodes): # collect trajectories\n",
    "    total_steps = 0\n",
    "    trajectories = []\n",
    "    for e in range(episodes):\n",
    "        observes, actions, rewards, infos = evaluate_episode(env, agent)\n",
    "        total_steps += observes.shape[0]\n",
    "        trajectory = {'observes': observes,\n",
    "                      'actions': actions,\n",
    "                      'rewards': rewards,\n",
    "                      'infos': infos}\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories\n",
    "\n",
    "def run_episode(env, policy, animate=False, target_vel=1.0): # Run policy and collect (state, action, reward) pairs\n",
    "    obs = env.reset()\n",
    "    observes, actions, rewards, infos = [], [], [], []\n",
    "    done = False\n",
    "    while not done:\n",
    "        \n",
    "        obs = obs.astype(np.float32).reshape((1, -1))\n",
    "        observes.append(obs)\n",
    "        \n",
    "        action = agent.get_action(obs)\n",
    "        actions.append(action)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        reward = target_vel*info['reward_run'] + info['reward_ctrl'] + info['alive_bonus']\n",
    "        \n",
    "        if not isinstance(reward, float):\n",
    "            reward = np.asscalar(reward)\n",
    "        rewards.append(reward)\n",
    "        infos.append(info)\n",
    "        \n",
    "    return (np.concatenate(observes), np.array(actions), np.array(rewards, dtype=np.float32), infos)\n",
    "\n",
    "def run_policy(env, agent, episodes, target_vel=1.0): # collect trajectories\n",
    "    total_steps = 0\n",
    "    trajectories = []\n",
    "    for e in range(episodes):\n",
    "        observes, actions, rewards, infos = run_episode(env, agent, target_vel=target_vel)\n",
    "        total_steps += observes.shape[0]\n",
    "        trajectory = {'observes': observes,\n",
    "                      'actions': actions,\n",
    "                      'rewards': rewards,\n",
    "                      'infos': infos}\n",
    "        trajectories.append(trajectory)\n",
    "    return trajectories\n",
    "        \n",
    "def add_value(trajectories, val_func): # Add value estimation for each trajectories\n",
    "    for trajectory in trajectories:\n",
    "        observes = trajectory['observes']\n",
    "        values = val_func.get_value(observes)\n",
    "        trajectory['values'] = values\n",
    "\n",
    "def add_gae(trajectories, gamma=0.99, lam=0.98): # generalized advantage estimation (for training stability)\n",
    "    for trajectory in trajectories:\n",
    "        rewards = trajectory['rewards']\n",
    "        values = trajectory['values']\n",
    "        \n",
    "        # temporal differences\n",
    "        tds = rewards + np.append(values[1:],0) * gamma - values\n",
    "        advantages = np.zeros_like(tds)\n",
    "        advantage = 0\n",
    "        for t in reversed(range(len(tds))):\n",
    "            advantage = tds[t] + lam*gamma*advantage\n",
    "            advantages[t] = advantage\n",
    "        trajectory['advantages'] = advantages\n",
    "\n",
    "def add_rets(trajectories, gamma=0.99): # compute the returns\n",
    "    for trajectory in trajectories:\n",
    "        rewards = trajectory['rewards']\n",
    "        \n",
    "        returns = np.zeros_like(rewards)\n",
    "        ret = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            ret = rewards[t] + gamma*ret\n",
    "            returns[t] = ret            \n",
    "        trajectory['returns'] = returns\n",
    "\n",
    "def build_train_set(trajectories):\n",
    "    observes = np.concatenate([t['observes'] for t in trajectories])\n",
    "    actions = np.concatenate([t['actions'] for t in trajectories])\n",
    "    returns = np.concatenate([t['returns'] for t in trajectories])\n",
    "    advantages = np.concatenate([t['advantages'] for t in trajectories])\n",
    "\n",
    "    # Normalization of advantages \n",
    "    # In baselines, which is a github repo including implementation of PPO coded by OpenAI, \n",
    "    # all policy gradient methods use advantage normalization trick as belows.\n",
    "    # The insight under this trick is that it tries to move policy parameter towards locally maximum point.\n",
    "    # Sometimes, this trick doesnot work.\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-6)\n",
    "\n",
    "    return observes, actions, advantages, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[0/1000] return : 71.815, value loss : 302.161, policy loss : -0.006, policy kl : 0.00010, policy entropy : 8.502\n",
      "[10/1000] return : 33.231, value loss : 243.454, policy loss : -0.018, policy kl : 0.01180, policy entropy : 8.540\n",
      "[20/1000] return : 27.116, value loss : 242.193, policy loss : -0.019, policy kl : 0.00651, policy entropy : 8.565\n",
      "[30/1000] return : 21.147, value loss : 394.074, policy loss : -0.017, policy kl : 0.00758, policy entropy : 8.480\n",
      "[40/1000] return : 22.345, value loss : 501.016, policy loss : -0.016, policy kl : 0.00612, policy entropy : 8.447\n",
      "[50/1000] return : 14.579, value loss : 543.709, policy loss : -0.015, policy kl : 0.00394, policy entropy : 8.417\n",
      "[60/1000] return : 7.131, value loss : 476.090, policy loss : -0.015, policy kl : 0.00717, policy entropy : 8.395\n",
      "[70/1000] return : 2.226, value loss : 419.303, policy loss : -0.015, policy kl : 0.00743, policy entropy : 8.316\n",
      "[80/1000] return : -0.892, value loss : 374.673, policy loss : -0.015, policy kl : 0.00590, policy entropy : 8.263\n",
      "[90/1000] return : -3.794, value loss : 343.601, policy loss : -0.015, policy kl : 0.01063, policy entropy : 8.258\n",
      "[100/1000] return : -8.291, value loss : 326.898, policy loss : -0.015, policy kl : 0.00706, policy entropy : 8.211\n",
      "[110/1000] return : -11.827, value loss : 328.539, policy loss : -0.015, policy kl : 0.00845, policy entropy : 8.161\n",
      "[120/1000] return : -13.401, value loss : 334.292, policy loss : -0.014, policy kl : 0.00830, policy entropy : 8.114\n",
      "[130/1000] return : -9.061, value loss : 289.755, policy loss : -0.014, policy kl : 0.00782, policy entropy : 7.974\n",
      "[140/1000] return : -13.823, value loss : 226.446, policy loss : -0.015, policy kl : 0.00972, policy entropy : 7.931\n",
      "[150/1000] return : -13.829, value loss : 168.834, policy loss : -0.015, policy kl : 0.00698, policy entropy : 7.873\n",
      "[160/1000] return : -13.390, value loss : 174.383, policy loss : -0.015, policy kl : 0.00992, policy entropy : 7.757\n",
      "[170/1000] return : -14.637, value loss : 188.063, policy loss : -0.015, policy kl : 0.00818, policy entropy : 7.697\n",
      "[180/1000] return : -14.669, value loss : 205.667, policy loss : -0.015, policy kl : 0.00935, policy entropy : 7.542\n",
      "[190/1000] return : -15.410, value loss : 217.787, policy loss : -0.015, policy kl : 0.00762, policy entropy : 7.505\n",
      "[200/1000] return : -13.666, value loss : 224.165, policy loss : -0.016, policy kl : 0.00598, policy entropy : 7.455\n",
      "[210/1000] return : -27.162, value loss : 215.450, policy loss : -0.016, policy kl : 0.01129, policy entropy : 7.363\n",
      "[220/1000] return : -38.755, value loss : 206.748, policy loss : -0.016, policy kl : 0.00891, policy entropy : 7.251\n",
      "[230/1000] return : -60.966, value loss : 197.021, policy loss : -0.016, policy kl : 0.00649, policy entropy : 7.152\n",
      "[240/1000] return : -83.022, value loss : 197.068, policy loss : -0.016, policy kl : 0.01000, policy entropy : 7.070\n",
      "[250/1000] return : -114.915, value loss : 201.166, policy loss : -0.016, policy kl : 0.00657, policy entropy : 7.043\n",
      "[260/1000] return : -151.421, value loss : 208.855, policy loss : -0.016, policy kl : 0.00856, policy entropy : 6.875\n",
      "[270/1000] return : -160.975, value loss : 208.256, policy loss : -0.016, policy kl : 0.00592, policy entropy : 6.801\n",
      "[280/1000] return : -145.877, value loss : 209.501, policy loss : -0.016, policy kl : 0.00715, policy entropy : 6.549\n",
      "[290/1000] return : -160.909, value loss : 208.989, policy loss : -0.015, policy kl : 0.00840, policy entropy : 6.395\n",
      "[300/1000] return : -170.914, value loss : 203.906, policy loss : -0.015, policy kl : 0.00811, policy entropy : 6.388\n",
      "[310/1000] return : -201.002, value loss : 204.284, policy loss : -0.014, policy kl : 0.01042, policy entropy : 6.272\n",
      "[320/1000] return : -227.940, value loss : 204.006, policy loss : -0.014, policy kl : 0.00871, policy entropy : 6.149\n",
      "[330/1000] return : -241.940, value loss : 207.857, policy loss : -0.014, policy kl : 0.00885, policy entropy : 6.057\n",
      "[340/1000] return : -266.609, value loss : 206.808, policy loss : -0.013, policy kl : 0.00779, policy entropy : 5.960\n",
      "[350/1000] return : -290.181, value loss : 210.492, policy loss : -0.013, policy kl : 0.00891, policy entropy : 5.872\n",
      "[360/1000] return : -302.914, value loss : 209.220, policy loss : -0.013, policy kl : 0.00653, policy entropy : 5.725\n",
      "[370/1000] return : -346.283, value loss : 212.023, policy loss : -0.013, policy kl : 0.00773, policy entropy : 5.472\n",
      "[380/1000] return : -428.489, value loss : 217.897, policy loss : -0.013, policy kl : 0.01287, policy entropy : 5.249\n",
      "[390/1000] return : -472.659, value loss : 232.706, policy loss : -0.013, policy kl : 0.00712, policy entropy : 5.255\n",
      "[400/1000] return : -528.015, value loss : 249.610, policy loss : -0.012, policy kl : 0.01166, policy entropy : 5.072\n",
      "[410/1000] return : -570.194, value loss : 263.566, policy loss : -0.012, policy kl : 0.00912, policy entropy : 5.004\n",
      "[420/1000] return : -612.400, value loss : 277.517, policy loss : -0.012, policy kl : 0.00790, policy entropy : 4.925\n",
      "[430/1000] return : -645.258, value loss : 308.082, policy loss : -0.012, policy kl : 0.00928, policy entropy : 4.831\n",
      "[440/1000] return : -670.134, value loss : 318.813, policy loss : -0.012, policy kl : 0.00999, policy entropy : 4.619\n",
      "[450/1000] return : -724.659, value loss : 327.205, policy loss : -0.012, policy kl : 0.01115, policy entropy : 4.461\n",
      "[460/1000] return : -788.498, value loss : 358.554, policy loss : -0.013, policy kl : 0.00821, policy entropy : 4.331\n",
      "[470/1000] return : -841.077, value loss : 378.743, policy loss : -0.013, policy kl : 0.01242, policy entropy : 4.257\n",
      "[480/1000] return : -862.365, value loss : 387.133, policy loss : -0.013, policy kl : 0.00966, policy entropy : 4.099\n",
      "[490/1000] return : -903.610, value loss : 377.398, policy loss : -0.014, policy kl : 0.01043, policy entropy : 3.928\n",
      "[500/1000] return : -938.266, value loss : 376.587, policy loss : -0.014, policy kl : 0.01051, policy entropy : 3.708\n",
      "[510/1000] return : -932.447, value loss : 380.255, policy loss : -0.014, policy kl : 0.01014, policy entropy : 3.408\n",
      "[520/1000] return : -932.450, value loss : 380.279, policy loss : -0.014, policy kl : 0.00915, policy entropy : 3.291\n",
      "[530/1000] return : -975.886, value loss : 359.098, policy loss : -0.015, policy kl : 0.01137, policy entropy : 3.005\n",
      "[540/1000] return : -994.335, value loss : 358.826, policy loss : -0.015, policy kl : 0.01329, policy entropy : 2.924\n",
      "[550/1000] return : -986.826, value loss : 355.680, policy loss : -0.015, policy kl : 0.01133, policy entropy : 2.820\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-2d26c5c2daf5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnupdates\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mtrajectories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_vel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0madd_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0madd_gae\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrajectories\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-5fcc5fa54691>\u001b[0m in \u001b[0;36mrun_policy\u001b[0;34m(env, agent, episodes, target_vel)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mtrajectories\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mobserves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_vel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_vel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mtotal_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mobserves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         trajectory = {'observes': observes,\n",
      "\u001b[0;32m<ipython-input-3-5fcc5fa54691>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, policy, animate, target_vel)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mobserves\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-6ad291ac886c>\u001b[0m in \u001b[0;36mget_action\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# SAMPLE FROM POLICY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0msampled_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msampled_action\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "envname='Walker2d-v2'\n",
    "env = gym.make(envname)\n",
    "\n",
    "# reward_run_coeff = 0.025 # Expert:1.0, Novice:0.05, Crazy(Tabu):-1.0\n",
    "target_vel = -1.0\n",
    "\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)\n",
    "env.seed(seed=seed)\n",
    "\n",
    "obs_space = env.observation_space\n",
    "act_space= env.action_space\n",
    "\n",
    "obs_dim = obs_space.shape[0]\n",
    "act_dim = act_space.shape[0]\n",
    "\n",
    "agent = PPOAgent_gaussian(obs_dim, act_dim, epochs=10, hdim=64, policy_lr=1e-4, value_lr=1e-3, max_std=2.0,\n",
    "                          clip_range=0.2, seed=seed)\n",
    "\n",
    "avg_return_list = deque(maxlen=100)\n",
    "avg_pol_loss_list = deque(maxlen=100)\n",
    "avg_val_loss_list = deque(maxlen=100)\n",
    "\n",
    "episode_size = 10\n",
    "batch_size = 64\n",
    "nupdates = 1000\n",
    "\n",
    "for update in range(nupdates+1):\n",
    "\n",
    "    trajectories = run_policy(env, agent, episode_size, target_vel=target_vel)\n",
    "    add_value(trajectories, agent)\n",
    "    add_gae(trajectories)\n",
    "    add_rets(trajectories)\n",
    "    observes, actions, advantages, returns = build_train_set(trajectories)\n",
    "\n",
    "    pol_loss, val_loss, kl, entropy = agent.update(observes, actions, advantages, returns, batch_size=batch_size)\n",
    "\n",
    "    avg_pol_loss_list.append(pol_loss)\n",
    "    avg_val_loss_list.append(val_loss)\n",
    "    \n",
    "    trajectories_eval = evaluate_policy(env, agent, 1)\n",
    "    avg_return_list.append([np.sum(t['rewards']) for t in trajectories_eval])\n",
    "    if (update%10) == 0:\n",
    "        print('[{}/{}] return : {:.3f}, value loss : {:.3f}, policy loss : {:.3f}, policy kl : {:.5f}, policy entropy : {:.3f}'.format(\n",
    "            update, nupdates, np.mean(avg_return_list), np.mean(avg_val_loss_list), np.mean(avg_pol_loss_list), kl, entropy))\n",
    "        \n",
    "    if (np.mean(avg_return_list) > 3000): # Threshold return to success cartpole\n",
    "        print('[{}/{}] return : {:.3f}, value loss : {:.3f}, policy loss : {:.3f}'.format(update,nupdates, np.mean(avg_return_list), np.mean(avg_val_loss_list), np.mean(avg_pol_loss_list)))\n",
    "        print('The problem is solved with {} episodes'.format(update*episode_size))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1064.0099\n",
      "crazy\n"
     ]
    }
   ],
   "source": [
    "demonstrations = evaluate_policy(env, agent, 50)\n",
    "avg_ret = np.mean([np.sum(d['rewards']) for d in demonstrations])\n",
    "print(avg_ret)\n",
    "import pickle\n",
    "if target_vel == 1.0:\n",
    "    with open('./demos/'+envname+'-expert.pkl', 'wb') as f:\n",
    "        pickle.dump([demonstrations], f)\n",
    "        f.close()\n",
    "        print('expert')\n",
    "        \n",
    "if  1.0 > target_vel and target_vel > 0:\n",
    "    with open('./demos/'+envname+'-novice.pkl', 'wb') as f:\n",
    "        pickle.dump([demonstrations], f)\n",
    "        f.close()\n",
    "        print('novice')\n",
    "        \n",
    "if target_vel < 0:\n",
    "    with open('./demos/'+envname+'-crazy.pkl', 'wb') as f:\n",
    "        pickle.dump([demonstrations], f)\n",
    "        f.close()\n",
    "        print('crazy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Imports specifically so we can render outputs in Jupyter.\n",
    "# from JSAnimation.IPython_display import display_animation\n",
    "# from matplotlib import animation\n",
    "# from IPython.display import display\n",
    "# from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# def display_frames_as_gif(frames):\n",
    "#     patch = plt.imshow(frames[0])\n",
    "#     plt.axis('off')\n",
    "#     def animate(i):\n",
    "#         patch.set_data(frames[i])\n",
    "\n",
    "#     anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=5)\n",
    "#     display(display_animation(anim, default_mode='loop'))\n",
    "\n",
    "# env = gym.make(envname)\n",
    "# imgs = []\n",
    "# obs = env.reset()\n",
    "# env.render('rgb_array')\n",
    "# env.env.viewer.cam.distance = 10.0\n",
    "# done = False\n",
    "# while not done:\n",
    "# #     print('move')\n",
    "#     imgs.append(env.render('rgb_array'))\n",
    "#     obs = obs.astype(np.float32).reshape((1, -1))\n",
    "\n",
    "#     action = agent.control(obs)\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "\n",
    "# display_frames_as_gif(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(imgs[0])\n",
    "# plt.show()\n",
    "# plt.imshow(imgs[10])\n",
    "# plt.show()\n",
    "# plt.imshow(imgs[20])\n",
    "# plt.show()\n",
    "# plt.imshow(imgs[30])\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
