{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version is [1.10.0].\n"
     ]
    }
   ],
   "source": [
    "import nbloader,os,warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from sklearn.utils import shuffle\n",
    "from util import gpusession,create_gradient_clipping,data4imitation_noisy,data4imitation_expert_crazy,plot_1dRegData,print_n_txt\n",
    "%matplotlib inline  \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "if __name__ == \"__main__\":\n",
    "    print (\"TensorFlow version is [%s].\"%(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define MLP Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlp_reg_class defined.\n"
     ]
    }
   ],
   "source": [
    "class mlp_imitation_class(object):\n",
    "    def __init__(self,_name='MLP',_envname='HalfCheetah-v2',_xdim=1,_ydim=1,_hdims=[64,64]\n",
    "                 ,_actv=tf.nn.tanh,_bn=slim.batch_norm\n",
    "                 ,_l2_reg_coef=1e-5,_GPU_ID=0,_VERBOSE=True):\n",
    "        self.name = _name\n",
    "        self.envname = _envname\n",
    "        self.env = gym.make(_envname)\n",
    "        self.xdim = _xdim \n",
    "        self.ydim = _ydim\n",
    "        self.hdims = _hdims\n",
    "        self.actv = _actv\n",
    "        self.bn = _bn\n",
    "        self.l2_reg_coef = _l2_reg_coef\n",
    "        self.GPU_ID = _GPU_ID\n",
    "        self.VERBOSE = _VERBOSE\n",
    "#         with tf.device('/device:GPU:%d'%(self.GPU_ID)):\n",
    "        # Build model\n",
    "        self.build_model()\n",
    "        # Build graph\n",
    "        self.build_graph()\n",
    "        # Check params\n",
    "        self.check_params()\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.x = tf.placeholder(dtype=tf.float32,shape=[None,self.xdim]) # Input [N x xdim]\n",
    "        self.y = tf.placeholder(dtype=tf.float32,shape=[None,self.ydim]) # Output [N x ydim]\n",
    "        self.kp = tf.placeholder(dtype=tf.float32,shape=[]) # Keep probability \n",
    "        self.lr = tf.placeholder(dtype=tf.float32,shape=[]) # Learning rate\n",
    "        self.is_training = tf.placeholder(dtype=tf.bool,shape=[]) # Training flag\n",
    "        # Initializers\n",
    "        self.fully_init  = tf.random_normal_initializer(stddev=0.01)\n",
    "        self.bias_init   = tf.constant_initializer(0.)\n",
    "        self.bn_init     = {'beta': tf.constant_initializer(0.),\n",
    "                           'gamma': tf.random_normal_initializer(1., 0.01)}\n",
    "        self.bn_params   = {'is_training':self.is_training,'decay':0.9,'epsilon':1e-5,\n",
    "                           'param_initializers':self.bn_init,'updates_collections':None}\n",
    "        # Build graph\n",
    "        with tf.variable_scope(self.name,reuse=False) as scope:\n",
    "            with slim.arg_scope([slim.fully_connected]\n",
    "                                ,activation_fn=self.actv\n",
    "                                ,weights_initializer=self.fully_init\n",
    "                                ,biases_initializer=self.bias_init\n",
    "                                ,normalizer_fn=self.bn,normalizer_params=self.bn_params\n",
    "                                ,weights_regularizer=None):\n",
    "                _net = self.x # Input [N x xdim]\n",
    "                for h_idx in range(len(self.hdims)): # Loop over hidden layers\n",
    "                    _hdim = self.hdims[h_idx]\n",
    "                    _net = slim.fully_connected(_net,_hdim,scope='lin'+str(h_idx))\n",
    "                    _net = slim.dropout(_net,keep_prob=self.kp,is_training=self.is_training\n",
    "                                        ,scope='dr'+str(h_idx))  \n",
    "                self.feat = _net # Feature [N x Q]\n",
    "                self.out = slim.fully_connected(self.feat,self.ydim,activation_fn=None\n",
    "                                                ,scope='out') # [N x D]\n",
    "    def build_graph(self):\n",
    "        # L2 fitting loss\n",
    "        self._loss_fit = tf.reduce_sum(tf.pow(self.out-self.y,2),axis=1) # [N x 1]\n",
    "        self.loss_fit = tf.reduce_mean(self._loss_fit) # [1]\n",
    "        # Weight decay\n",
    "        _g_vars = tf.trainable_variables()\n",
    "        self.c_vars = [var for var in _g_vars if '%s/'%(self.name) in var.name]\n",
    "        self.l2_reg = self.l2_reg_coef*tf.reduce_sum(tf.stack([tf.nn.l2_loss(v) for v in self.c_vars])) # [1]\n",
    "        self.loss_total = self.loss_fit + self.l2_reg # [1]\n",
    "        # Optimizer\n",
    "        USE_ADAM = False\n",
    "        if USE_ADAM:\n",
    "            self.optm = tf.train.AdamOptimizer(learning_rate=self.lr,beta1=0.9,beta2=0.999\n",
    "                                               ,epsilon=1e-0).minimize(self.loss_total)\n",
    "        else:\n",
    "            self.optm = tf.train.MomentumOptimizer(learning_rate=self.lr\n",
    "                                                   ,momentum=0.0).minimize(self.loss_total)\n",
    "     # Check parameters\n",
    "    def check_params(self):\n",
    "        _g_vars = tf.global_variables()\n",
    "        self.g_vars = [var for var in _g_vars if '%s/'%(self.name) in var.name]\n",
    "        if self.VERBOSE:\n",
    "            print (\"==== Global Variables ====\")\n",
    "        for i in range(len(self.g_vars)):\n",
    "            w_name  = self.g_vars[i].name\n",
    "            w_shape = self.g_vars[i].get_shape().as_list()\n",
    "            if self.VERBOSE:\n",
    "                print (\" [%02d] Name:[%s] Shape:[%s]\" % (i,w_name,w_shape))\n",
    "                \n",
    "    # Sampler\n",
    "    def sampler(self,_sess,_x):\n",
    "        outVal = _sess.run(self.out,feed_dict={self.x:_x,self.kp:1.0,self.is_training:False})\n",
    "        return outVal\n",
    "    \n",
    "    # Save \n",
    "    def save(self,_sess,_savename=None):\n",
    "        \"\"\" Save name \"\"\"\n",
    "        if _savename==None:\n",
    "            _savename='../net/net_%s.npz'%(self.name)\n",
    "        \"\"\" Get global variables \"\"\"\n",
    "        self.g_wnames,self.g_wvals,self.g_wshapes = [],[],[]\n",
    "        for i in range(len(self.g_vars)):\n",
    "            curr_wname = self.g_vars[i].name\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==curr_wname][0]\n",
    "            curr_wval  = _sess.run(curr_wvar)\n",
    "            curr_wval_sqz  = curr_wval.squeeze()\n",
    "            self.g_wnames.append(curr_wname)\n",
    "            self.g_wvals.append(curr_wval_sqz)\n",
    "            self.g_wshapes.append(curr_wval.shape)\n",
    "        \"\"\" Save \"\"\"\n",
    "        np.savez(_savename,g_wnames=self.g_wnames,g_wvals=self.g_wvals,g_wshapes=self.g_wshapes)\n",
    "        if self.VERBOSE:\n",
    "            print (\"[%s] Saved. Size is [%.4f]MB\" % \n",
    "                   (_savename,os.path.getsize(_savename)/1000./1000.))\n",
    "            \n",
    "    # Save \n",
    "    def save_final(self,_sess,_savename=None):\n",
    "        \"\"\" Save name \"\"\"\n",
    "        if _savename==None:\n",
    "            _savename='../net/net_%s_final.npz'%(self.name)\n",
    "        \"\"\" Get global variables \"\"\"\n",
    "        self.g_wnames,self.g_wvals,self.g_wshapes = [],[],[]\n",
    "        for i in range(len(self.g_vars)):\n",
    "            curr_wname = self.g_vars[i].name\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==curr_wname][0]\n",
    "            curr_wval  = _sess.run(curr_wvar)\n",
    "            curr_wval_sqz  = curr_wval.squeeze()\n",
    "            self.g_wnames.append(curr_wname)\n",
    "            self.g_wvals.append(curr_wval_sqz)\n",
    "            self.g_wshapes.append(curr_wval.shape)\n",
    "        \"\"\" Save \"\"\"\n",
    "        np.savez(_savename,g_wnames=self.g_wnames,g_wvals=self.g_wvals,g_wshapes=self.g_wshapes)\n",
    "        print (\"[%s] Saved. Size is [%.4f]MB\" % \n",
    "               (_savename,os.path.getsize(_savename)/1000./1000.))\n",
    "    \n",
    "    # Restore\n",
    "    def restore(self,_sess,_loadname=None):\n",
    "        if _loadname==None:\n",
    "            _loadname='../net/net_%s_final.npz'%(self.name)\n",
    "        l = np.load(_loadname)\n",
    "        g_wnames = l['g_wnames']\n",
    "        g_wvals  = l['g_wvals']\n",
    "        g_wshapes = l['g_wshapes']\n",
    "        for widx,wname in enumerate(g_wnames):\n",
    "            curr_wvar  = [v for v in tf.global_variables() if v.name==wname][0]\n",
    "            _sess.run(tf.assign(curr_wvar,g_wvals[widx].reshape(g_wshapes[widx])))\n",
    "        if self.VERBOSE:\n",
    "            print (\"Weight restored from [%s] Size is [%.4f]MB\" % \n",
    "                   (_loadname,os.path.getsize(_loadname)/1000./1000.))\n",
    "    \n",
    "    # Save to mat file\n",
    "    def save2mat(self,_xdata='',_ydata='',_yref=''):\n",
    "        # Save weights to mat file so that MATLAB can use it.\n",
    "        npzPath = '../net/net_%s.npz'%(self.name)\n",
    "        l = np.load(npzPath)\n",
    "        g_wnames = l['g_wnames']\n",
    "        g_wvals  = l['g_wvals']\n",
    "        g_wshapes = l['g_wshapes']\n",
    "        D = {}\n",
    "        for widx,wname in enumerate(g_wnames):\n",
    "            cName = wname.replace(':0','')\n",
    "            cName = cName.replace(self.name+'/','')\n",
    "            cName = cName.replace('/','_')\n",
    "            cVal = g_wvals[widx].reshape(g_wshapes[widx])\n",
    "            D[cName] = cVal\n",
    "            # if self.VERBOSE: print (\"name is [%s] shape is %s.\"%(cName,cVal.shape,))\n",
    "        # Save data\n",
    "        if _xdata!='': D['xdata']=_xdata\n",
    "        if _ydata!='': D['ydata']=_ydata\n",
    "        if _yref!='': D['yref']=_yref\n",
    "        # Save dictionary D to the mat file\n",
    "        matPath = '../data/net_%s.mat'%(self.name)\n",
    "        sio.savemat(matPath,D)\n",
    "        if self.VERBOSE: print (\"[%s] saved.\"%(matPath))\n",
    "        \n",
    "    # Save to mat file\n",
    "    def save2mat_final(self,_xdata='',_ydata='',_yref=''):\n",
    "        # Save weights to mat file so that MATLAB can use it.\n",
    "        npzPath = '../net/net_%s_final.npz'%(self.name)\n",
    "        l = np.load(npzPath)\n",
    "        g_wnames = l['g_wnames']\n",
    "        g_wvals  = l['g_wvals']\n",
    "        g_wshapes = l['g_wshapes']\n",
    "        D = {}\n",
    "        for widx,wname in enumerate(g_wnames):\n",
    "            cName = wname.replace(':0','')\n",
    "            cName = cName.replace(self.name+'/','')\n",
    "            cName = cName.replace('/','_')\n",
    "            cVal = g_wvals[widx].reshape(g_wshapes[widx])\n",
    "            D[cName] = cVal\n",
    "            if self.VERBOSE:\n",
    "                print (\"name is [%s] shape is %s.\"%(cName,cVal.shape,))\n",
    "        # Save data\n",
    "        if _xdata!='': D['xdata']=_xdata\n",
    "        if _ydata!='': D['ydata']=_ydata\n",
    "        if _yref!='': D['yref']=_yref\n",
    "        # Save dictionary D to the mat file        \n",
    "        matPath = '../data/net_%s_final.mat'%(self.name)\n",
    "        sio.savemat(matPath,D)\n",
    "        print (\"[%s] Saved. Size is [%.4f]MB\" % \n",
    "               (matPath,os.path.getsize(matPath)/1000./1000.))\n",
    "        \n",
    "    # Train\n",
    "    def train(self,_sess,_x,_y,_yref='',_lr=1e-3,_batchSize=512,_maxEpoch=1e4,_kp=1.0\n",
    "              ,_LR_SCHEDULE=True\n",
    "              ,_PRINT_EVERY=20,_PLOT_EVERY=20\n",
    "              ,_SAVE_TXT=True,_SAVE_BEST_NET=True,_SAVE_FINAL=True):\n",
    "        # Reference training data \n",
    "        _x_train,_y_train = _x,_y\n",
    "        \n",
    "        # Iterate\n",
    "        if _PRINT_EVERY == 0: print_period = 0\n",
    "        else: print_period = _maxEpoch//_PRINT_EVERY\n",
    "        if _PLOT_EVERY == 0: plot_period = 0\n",
    "        else: plot_period = _maxEpoch//_PLOT_EVERY\n",
    "        \n",
    "        maxIter = max(_x_train.shape[0]//_batchSize, 1)\n",
    "        bestLossVal = np.inf\n",
    "        if _SAVE_TXT:\n",
    "            txtName = ('../res/imitation_%s.txt'%(self.name));f = open(txtName,'w') # Open txt file\n",
    "            print_n_txt(_f=f,_chars='Text name: '+txtName,_DO_PRINT=True)\n",
    "        for epoch in range((int)(_maxEpoch)+1): # For every epoch\n",
    "            _x_train,_y_train = shuffle(_x_train,_y_train)\n",
    "            for iter in range(maxIter): # For every iteration\n",
    "                start,end = iter*_batchSize,(iter+1)*_batchSize\n",
    "                if _LR_SCHEDULE:\n",
    "                    if epoch < 0.5*_maxEpoch:\n",
    "                        lr_use = _lr\n",
    "                    elif epoch < 0.75*_maxEpoch:\n",
    "                        lr_use = _lr/5.\n",
    "                    else:\n",
    "                        lr_use = _lr/10.\n",
    "                else:\n",
    "                    lr_use = _lr\n",
    "                feeds = {self.x:_x_train[start:end,:],self.y:_y_train[start:end,:]\n",
    "                         ,self.kp:_kp,self.lr:lr_use,self.is_training:True}\n",
    "                # Optimize \n",
    "                _sess.run(self.optm,feeds)\n",
    "\n",
    "            # Track the Best result\n",
    "            BEST_FLAG = False\n",
    "            check_period = _maxEpoch//100\n",
    "            if (epoch%check_period)==0:\n",
    "                # Feed total dataset \n",
    "                feeds = {self.x:_x,self.y:_y,self.kp:1.0,self.is_training:False}\n",
    "                opers = [self.loss_total,self.loss_fit,self.l2_reg]\n",
    "                lossVal,loss_fit,l2_reg = _sess.run(opers,feeds)\n",
    "                if (lossVal < bestLossVal) & (epoch >= 3):\n",
    "                    bestLossVal = lossVal\n",
    "                    BEST_FLAG = True\n",
    "                    if _SAVE_BEST_NET:\n",
    "                        self.save(_sess) # Save the current best model \n",
    "                        self.save2mat(_xdata=_x,_ydata=_y,_yref=_yref)\n",
    "            \n",
    "            # Print current result \n",
    "            if (print_period!=0) and ((epoch%print_period)==0 or (epoch==(_maxEpoch-1))): # Print \n",
    "                feeds = {self.x:_x,self.y:_y,self.kp:1.0,self.is_training:False}\n",
    "                opers = [self.loss_total,self.loss_fit,self.l2_reg]\n",
    "                lossVal,loss_fit,l2_reg = _sess.run(opers,feeds)\n",
    "                if _SAVE_TXT:\n",
    "                    strTemp = (\"[%d/%d] loss:%.3f(fit:%.3f+l2:%.3f) bestLoss:%.3f\"\n",
    "                               %(epoch,_maxEpoch,lossVal,loss_fit,l2_reg,bestLossVal))\n",
    "                    print_n_txt(_f=f,_chars=strTemp,_DO_PRINT=self.VERBOSE)\n",
    "                else:\n",
    "                    if self.VERBOSE:\n",
    "                        print (\"[%d/%d] loss:%.3f(fit:%.3f+l2:%.3f) bestLoss:%.3f\"\n",
    "                                   %(epoch,_maxEpoch,lossVal,loss_fit,l2_reg,bestLossVal))\n",
    "\n",
    "            # Plot current result \n",
    "            if (plot_period!=0) and ((epoch%plot_period)==0 or (epoch==(_maxEpoch-1))): # Plot\n",
    "                # Get loss vals\n",
    "                feeds = {self.x:_x,self.y:_y,self.kp:1.0,self.is_training:False}\n",
    "                opers = [self.loss_total,self.loss_fit,self.l2_reg]\n",
    "                lossVal,loss_fit,l2_reg = _sess.run(opers,feeds)\n",
    "                # Output\n",
    "                ytest = self.sampler(_sess=_sess,_x=_x)\n",
    "                # Plot first dimensions of both input and output\n",
    "                x_plot,y_plot = _x[:,0],_y[:,0] # Traning data \n",
    "                plt.figure(figsize=(8,4));\n",
    "                plt.axis([np.min(x_plot),np.max(x_plot),np.min(y_plot)-0.1,np.max(y_plot)+0.1])\n",
    "                if _yref != '': plt.plot(x_plot,_yref[:,0],'r.') # Plot reference\n",
    "                plt.plot(x_plot,y_plot,'k.') # Plot training data\n",
    "                plt.plot(_x,ytest[:,0],'b.')\n",
    "                plt.title(\"[%d/%d] name:[%s] lossVal:[%.3e]\"%(epoch,_maxEpoch,self.name,lossVal)); \n",
    "                plt.show()\n",
    "        # Save final weights \n",
    "        if _SAVE_FINAL:\n",
    "            self.save_final(_sess)\n",
    "            self.save2mat_final(_xdata=_x,_ydata=_y,_yref=_yref)\n",
    "        \n",
    "    def evaluate_episode(self, _sess, animate=False): # Run policy and collect (state, action, reward) pairs\n",
    "        env = self.env\n",
    "        obs = env.reset()\n",
    "        observes, actions, rewards, infos = [], [], [], []\n",
    "        done = False\n",
    "        while not done:\n",
    "\n",
    "            obs = obs.astype(np.float32).reshape((1, -1))\n",
    "            observes.append(obs)\n",
    "\n",
    "            action = self.sampler(_sess=_sess,_x=obs).reshape((1,-1))\n",
    "            actions.append(action)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "\n",
    "            if not isinstance(reward, float):\n",
    "                reward = np.asscalar(reward)\n",
    "            rewards.append(reward)\n",
    "            infos.append(info)\n",
    "        \n",
    "        return (np.concatenate(observes), np.array(actions), np.array(rewards, dtype=np.float32), infos)\n",
    "\n",
    "    def evaluate_policy(self, _sess, episodes): # collect trajectories\n",
    "        total_steps = 0\n",
    "        trajectories = []\n",
    "        for e in range(episodes):\n",
    "            observes, actions, rewards, infos = self.evaluate_episode(_sess)\n",
    "            total_steps += observes.shape[0]\n",
    "            trajectory = {'observes': observes,\n",
    "                          'actions': actions,\n",
    "                          'rewards': rewards,\n",
    "                          'infos': infos}\n",
    "            trajectories.append(trajectory)\n",
    "        return trajectories\n",
    "    \n",
    "    # Test\n",
    "    def test(self,_sess,_PLOT_TRAIN=True,_PLOT_RES=True,_SAVE_FIG=False):\n",
    "        \n",
    "        demonstrations = self.evaluate_policy(_sess, 100)\n",
    "        avg_ret = np.mean([np.sum(d['rewards']) for d in demonstrations])\n",
    "        std_ret = np.std([np.sum(d['rewards']) for d in demonstrations])\n",
    "        print('Avg Return:%.02f (%.02f)'%(avg_ret,std_ret))\n",
    "        with open('../res/%s.pickle'%(self.name),'wb') as f:\n",
    "            pickle.dump({'avg_ret':avg_ret,'std_ret':std_ret},f)\n",
    "            f.close()\n",
    "        return avg_ret\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    print (\"mlp_reg_class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train MLP for Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-d4918aa270c4>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-d4918aa270c4>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    x,y,t=data4imitation_expert_crazy(,_n=2000,_oRate=oRate,measVar=measVar)\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Training data\n",
    "    oRate = [1.0,0.0]\n",
    "    measVar = 1e-8\n",
    "    x,y,t=data4imitation_expert_crazy(_n=2000,_oRate=oRate,measVar=measVar)\n",
    "    \n",
    "    # Make graph\n",
    "    tf.reset_default_graph(); sess = gpusession()\n",
    "    tf.set_random_seed(0); np.random.seed(0)\n",
    "    MLP =  mlp_imitation_class(_name='MLP_%s_E%02d_C%02d_var%.1e'%('HalfCheetah',oRate[0]*100,oRate[1]*100,measVar)\n",
    "                         ,_xdim=x.shape[1],_ydim=y.shape[1],_hdims=[128,128],_actv=tf.nn.tanh,_bn=None#slim.batch_norm\n",
    "                         ,_l2_reg_coef=1e-8,_VERBOSE=True)\n",
    "    sess.run(tf.global_variables_initializer()) # Initialize variables\n",
    "    \n",
    "    # Train \n",
    "    DO_TRAIN = True\n",
    "    if DO_TRAIN:\n",
    "        MLP.train(_sess=sess,_x=x,_y=y,_yref=t \n",
    "               ,_lr=1e-1,_batchSize=256,_maxEpoch=1e4,_kp=1.0\n",
    "               ,_LR_SCHEDULE=True\n",
    "               ,_PRINT_EVERY=20,_PLOT_EVERY=20\n",
    "               ,_SAVE_TXT=False,_SAVE_BEST_NET=False)\n",
    "        print (\"Train done.\") \n",
    "    else: \n",
    "        MLP.restore(sess)\n",
    "        print (\"Network restored.\")\n",
    "        \n",
    "    # Test \n",
    "    MLP.test(_sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
